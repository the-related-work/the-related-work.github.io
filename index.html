<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="(Yongchang Hao)[https://yongchanghao.github.io]" />
  <title>The Related Work</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">The Related Work</h1>
<p class="author">(Yongchang Hao)[https://yongchanghao.github.io]</p>
</header>
<h1 class="unnumbered" id="section">2021</h1>
<h2 class="unnumbered" id="may-22">May 22</h2>
<h3 class="unnumbered" id="paper:he2017decoding">Decoding with Value Networks for Neural Machine Translation</h3>
<h4 id="content">Content</h4>
<p>This work introduces <span class="smallcaps">NMT-VNN</span> <span class="citation" data-cites="he2017decoding">(He et al. 2017)</span>, which incorporates a value network in machine translation decoding. The value network <span class="math inline">\(v(X, Y)\)</span> is trained to distinguish the better partial decoding between two partial decoding <span class="math inline">\(Y^{p,1}\)</span> and <span class="math inline">\(Y^{p,2}\)</span> generated by beam search with a certain model <span class="math inline">\(\pi(y|X, Y)\)</span>. The discrimination is based on the average BLEU score of complete target sentences of <span class="math inline">\(Y^{p,1}\)</span> and <span class="math inline">\(Y^{p,2}\)</span>. At each step <span class="math inline">\(T\)</span> of the decoding phase, they maximize <span class="math display">\[\label{eq:he2017decoding:1}
    \alpha\frac{\sum_i^T \log P(y_i|X, Y_{&lt; i})}{T} + (1-\alpha) \log v(X, Y_{\le T})\]</span> where <span class="math inline">\(\alpha\)</span> is the balancing factor.</p>
<h4 id="comment">Comment</h4>
<p>The idea of this work is promising, where they intend to foresee the future result of current selection in machine translation.</p>
<p>We shall start our analysis by interpreting what <span class="math inline">\(v(X,Y)\)</span> stands for here. Derived from the original paper, they train it by minimizing <span class="math display">\[\label{eq:he2017decoding:2}
    \exp \left( \left(v(X, Y^{p,1}) - v(X, Y^{p,2})\right) \mathrm{sign}\left(\mathrm{FutureBLEU}(Y^*, Y^{p,2}) - \mathrm{FutureBLEU}(Y^*, Y^{p,1})\right) \right)\]</span> When <span class="math inline">\(v(X, Y)\)</span> can correctly assign values for arbitrary partial decoding, function <a href="#eq:he2017decoding:2" data-reference-type="ref" data-reference="eq:he2017decoding:2">[eq:he2017decoding:2]</a> reaches global minima. Thus this objective seems reasonable. We can further infer that considering the current <span class="math inline">\(v(X, Y_{&lt; T, i})\)</span> and <span class="math inline">\(v(X, Y_{&lt; T, j})\)</span> is sufficient to compare the future performance of <span class="math inline">\(Y_{&lt; T, i}\)</span> and <span class="math inline">\(Y_{&lt; T, j}\)</span>.</p>
<p>However, in their experiments, we see that the best values of <span class="math inline">\(\alpha\)</span></p>
<ol>
<li><p>are large (<span class="math inline">\(\ge 0.8\)</span>).</p></li>
<li><p>are different across tasks.</p></li>
</ol>
<p>This may indicate that, in practice, the value network <span class="math inline">\(v(X, Y)\)</span> plays a less important role than our expectation. The reasons may come from insufficient training of <span class="math inline">\(v(X,Y)\)</span>, or the inconsistent between</p>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-he2017decoding" class="csl-entry" role="doc-biblioentry">
He, Di, Hanqing Lu, Yingce Xia, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2017. <span>“Decoding with Value Networks for Neural Machine Translation.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2017/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf</a>.
</div>
</div>
</body>
</html>
